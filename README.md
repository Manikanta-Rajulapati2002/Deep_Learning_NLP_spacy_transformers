# ğŸ§  Home Assignment 4 â€“ CS5720: Neural Networks and Deep Learning  
**University of Central Missouri â€“ Spring 2025**  
**Student Name:** Manikanta Rajulapati

**Course:** CS5720  


---

## ğŸ“Œ Overview

This assignment includes implementations of key NLP and deep learning concepts:
- NLP text preprocessing (tokenization, stopword removal, stemming)
- Named Entity Recognition using SpaCy
- Scaled dot-product attention (core of Transformer models)
- Sentiment analysis using HuggingFaceâ€™s pre-trained models

Each question has been solved in Python using popular libraries such as **NLTK, SpaCy, NumPy, and HuggingFace Transformers**.

---

## âœ… Q1: NLP Preprocessing Pipeline

### ğŸ”§ Task:
Write a function that performs three basic NLP preprocessing steps:
1. **Tokenization** â€“ Splits a sentence into words and punctuation.
2. **Stopword Removal** â€“ Removes common low-information words (e.g., *the*, *in*, *are*).
3. **Stemming** â€“ Reduces words to their root form using PorterStemmer.

### ğŸ§ª Input Sentence:
"NLP techniques are used in virtual assistants like Alexa and Siri."

### ğŸ“¤ Expected Output:
- **Original Tokens**: All words and punctuation split from the sentence.
- **Tokens Without Stopwords**: Only meaningful words remain.
- **Stemmed Words**: Each word is reduced to its base/root form.

### ğŸ“˜ Conceptual Notes:
- **Stemming vs Lemmatization**: Stemming chops off word endings crudely, while lemmatization returns the dictionary form.  
- **Stopword Removal Use Case**: Useful for classification and topic modeling, but might harm performance in sentiment analysis where words like â€œnotâ€ are critical.

---

## âœ… Q2: Named Entity Recognition (NER) with SpaCy

### ğŸ”§ Task:
Use the `spaCy` NLP library to:
- Identify named entities from a sentence.
- Print each entityâ€™s text, label (e.g., PERSON, DATE), and character span.

### ğŸ§ª Input Sentence:
"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."

### ğŸ“¤ Expected Output:
Each named entity is printed with:
- **Text**: "Barack Obama"
- **Label**: PERSON, DATE, ORG, etc.
- **Start/End Char**: Position in the original sentence.

### ğŸ“˜ Conceptual Notes:
- **NER vs POS Tagging**: NER identifies real-world entities; POS tagging marks grammatical roles like noun, verb, etc.
- **Real-World Applications**: Search engines, financial news extraction, legal document summarization, and more.

---

## âœ… Q3: Scaled Dot-Product Attention

### ğŸ”§ Task:
Implement scaled dot-product attention â€” the mathematical foundation of the Transformer architecture.

### ğŸ“˜ Steps:
1. Compute **Q Â· Káµ€**
2. Scale by **âˆšd** (dimension of the key vectors)
3. Apply **softmax** to get attention weights
4. Multiply by **V** (Value matrix) to get the output

### ğŸ§ª Test Input:
```python
Q = [[1, 0, 1, 0], [0, 1, 0, 1]]
K = [[1, 0, 1, 0], [0, 1, 0, 1]]
V = [[1, 2, 3, 4], [5, 6, 7, 8]]
ğŸ“¤ Expected Output:
Attention Weights Matrix

Final Output Matrix (weighted sum of values)

ğŸ“˜ Conceptual Notes:
Why scale by âˆšd?: Prevents large dot products from dominating softmax â€” improves numerical stability.

Use in Self-Attention: Helps the model focus on contextually relevant parts of the input.

## ğŸ“Œ Overview

This project demonstrates the use of a **pre-trained sentiment analysis pipeline** from HuggingFaceâ€™s `transformers` library.  
Given an input sentence, the model predicts whether the sentiment is **positive** or **negative**, along with a **confidence score**.

---

## âœ… Task Description

### âœ”ï¸ Objective:
- Use HuggingFaceâ€™s `pipeline()` API to:
  - Load a pre-trained sentiment classifier
  - Analyze the sentiment of a provided sentence
  - Print the sentiment **label** and **confidence score**

### ğŸ§ª Input Sentence: "Despite the high price, the performance of the new MacBook is outstanding."

### ğŸ“¤ Expected Output:
Sentiment: POSITIVE
Confidence Score: 0.9987
