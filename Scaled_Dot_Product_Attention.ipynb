{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c816353-e359-4c1c-a0bc-0f86ae79bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[0.73105858 0.26894142]\n",
      " [0.26894142 0.73105858]]\n",
      "\n",
      "Output Matrix:\n",
      " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
      " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.07576569, 3.07576569, 4.07576569, 5.07576569],\n",
       "       [3.92423431, 4.92423431, 5.92423431, 6.92423431]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    # Step 1: Compute dot product Q · K^T\n",
    "    scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Step 2: Scale scores by sqrt(d), where d = dimension of key\n",
    "    d_k = K.shape[-1]\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scaled_scores, axis=-1)\n",
    "    \n",
    "    # Step 4: Multiply attention weights by V\n",
    "    output = np.dot(attention_weights, V)\n",
    "\n",
    "    print(\"Attention Weights:\\n\", attention_weights)\n",
    "    print(\"\\nOutput Matrix:\\n\", output)\n",
    "    return output\n",
    "\n",
    "# Test input\n",
    "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
    "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
    "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "\n",
    "# Run the attention function\n",
    "scaled_dot_product_attention(Q, K, V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25a27f-ae64-4793-ac47-1217710b97e8",
   "metadata": {},
   "source": [
    "# 1. Why do we divide the attention score by √d in the scaled dot-product attention formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86609a40-0241-4384-9f72-15f274352c4d",
   "metadata": {},
   "source": [
    "We divide by √d to prevent the dot product values from growing too large when the dimension of the key vectors (d) is large. Without this scaling, the softmax would produce very small gradients, leading to vanishing gradient problems and slower learning.\n",
    "\n",
    "This scaling helps maintain numerical stability and ensures effective gradient flow during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26e7f6-1130-40ac-addc-4c250565fd9d",
   "metadata": {},
   "source": [
    "# 2. How does self-attention help the model understand realtionships between words in a sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bb857-931c-4399-9b72-25e48dce52fd",
   "metadata": {},
   "source": [
    "Self-attention allows the model to:\n",
    "\n",
    "Compare each word with all other words in the sentence.\n",
    "\n",
    "Assign different weights to different words based on their relevance.\n",
    "\n",
    "This mechanism enables the model to:\n",
    "\n",
    "Capture long-range dependencies (e.g., pronouns and antecedents far apart),\n",
    "\n",
    "Understand context better, and\n",
    "\n",
    "Perform parallel computation, unlike RNNs.\n",
    "\n",
    "For example, in the sentence \"The cat that chased the mouse was hungry,\" self-attention helps associate \"was hungry\" with \"cat\" rather than \"mouse.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b33a94-5ccf-40ee-a9f0-e9f7523221e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sunny]",
   "language": "python",
   "name": "conda-env-sunny-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
